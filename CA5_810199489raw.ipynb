{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "D_iL22oYt0AN",
      "metadata": {
        "id": "D_iL22oYt0AN"
      },
      "source": [
        "**Fatemeh Mohammadi**\n",
        "\n",
        "**810199489**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I2WGNNJxMouq",
      "metadata": {
        "id": "I2WGNNJxMouq"
      },
      "source": [
        "# Introduction:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BeJRalltM0VU",
      "metadata": {
        "id": "BeJRalltM0VU"
      },
      "source": [
        "\n",
        "Convolutional Neural Networks (CNNs) and Natural Language Processing (NLP) are two pivotal areas in the field of artificial intelligence and machine learning. CNNs have revolutionized image and video processing by enabling machines to automatically learn to recognize patterns and features. NLP, on the other hand, focuses on the interaction between computers and human language, aiming to read, decipher, and understand the intricacies of human language.\n",
        "\n",
        "In recent years, the integration of these two domains has opened new frontiers in developing sophisticated models that can process and analyze multimedia data comprehensively. This integration is particularly beneficial for applications that involve both visual and textual data, such as social media analysis, where posts often include images, videos, and text.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gWLJAqMiNCy4",
      "metadata": {
        "id": "gWLJAqMiNCy4"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KQ3iQsE3NBA0",
      "metadata": {
        "id": "KQ3iQsE3NBA0"
      },
      "source": [
        "\n",
        "\n",
        "With the proliferation of social media platforms, there is an increasing amount of data being generated every day. This data can be a rich source of information for various applications, including sentiment analysis, trend detection, and public health monitoring. However, the sheer volume and unstructured nature of social media data present significant challenges in extracting meaningful insights.\n",
        "\n",
        "One critical application of social media analysis is the detection of suicidal ideation and mental health issues. Identifying individuals at risk can provide timely intervention opportunities and potentially save lives. Despite its importance, accurately detecting such content remains challenging due to the complexity and variability of natural language and the need for nuanced understanding of context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gxWrUvOdNEdG",
      "metadata": {
        "id": "gxWrUvOdNEdG"
      },
      "source": [
        "## Overview of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YkidrfeANIuA",
      "metadata": {
        "id": "YkidrfeANIuA"
      },
      "source": [
        "The dataset used in this project is sourced from social media platforms, specifically Twitter, and is referred to as the \"twitter-suicidal-data\" dataset. This dataset is curated to aid in the detection of suicidal ideation from user-generated posts on social media.\n",
        "\n",
        "#### Data Collection\n",
        "\n",
        "- **Source:** The data consists of posts from Twitter, a popular social media platform where users share short messages known as tweets.\n",
        "- **Timeframe:** The tweets were collected over a specified period to capture a variety of content that reflects different contexts and sentiments.\n",
        "- **Volume:** The dataset contains thousands of tweets, providing a substantial amount of data for training and evaluating machine learning models.\n",
        "\n",
        "#### Data Characteristics\n",
        "\n",
        "- **Textual Data:** Each entry in the dataset includes a tweet, which is a short text message. The tweets vary in length and linguistic style, including abbreviations, slang, and informal language typical of social media communication.\n",
        "- **Labels:** Each tweet is labeled with an indication of whether it reflects suicidal ideation or not. The labels are binary, with `1` indicating suicidal ideation and `0` indicating the absence of such content.\n",
        "\n",
        "#### Data Preprocessing\n",
        "\n",
        "Given the raw nature of social media data, several preprocessing steps are necessary to prepare the dataset for analysis:\n",
        "\n",
        "- **Tokenization:** Splitting the text into individual tokens (words, hashtags, mentions).\n",
        "- **Stop Words Removal:** Removing common words that do not contribute significantly to the meaning (e.g., \"and,\" \"the\").\n",
        "- **Stemming/Lemmatization:** Reducing words to their root forms to ensure uniformity (e.g., \"running\" to \"run\").\n",
        "- **Handling Special Characters and Emojis:** Removing or converting special characters and emojis to a format that can be processed by machine learning algorithms.\n",
        "- **Normalization:** Converting text to lowercase to maintain consistency.\n",
        "\n",
        "#### Challenges and Considerations\n",
        "\n",
        "- **Noise:** Social media data is inherently noisy, containing a lot of irrelevant information, misspellings, and informal language, which requires extensive cleaning.\n",
        "- **Contextual Understanding:** Detecting suicidal ideation is complex and requires understanding the context and subtleties of the language used.\n",
        "- **Privacy and Ethics:** Handling sensitive data related to mental health requires strict adherence to ethical guidelines to ensure privacy and confidentiality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eY99YE-VG1C9",
      "metadata": {
        "id": "eY99YE-VG1C9"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec65989e04597f0",
      "metadata": {
        "collapsed": true,
        "id": "fec65989e04597f0"
      },
      "outputs": [],
      "source": [
        "!pip install gensim emoji nltk tqdm seaborn torch torchsummary -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CIyRvbWV-9gK",
      "metadata": {
        "id": "CIyRvbWV-9gK"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import gensim\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import os\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "import nltk\n",
        "import emoji\n",
        "import re\n",
        "\n",
        "nltk.download([\"stopwords\", \"punkt\", \"wordnet\", \"averaged_perceptron_tagger\"])\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf946d026ed2d98e",
      "metadata": {
        "collapsed": false,
        "id": "cf946d026ed2d98e"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e83a8b72b644c2",
      "metadata": {
        "collapsed": false,
        "id": "3e83a8b72b644c2"
      },
      "source": [
        "## Model training config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0826c5b8eb52c9",
      "metadata": {
        "id": "2d0826c5b8eb52c9"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 4e-4\n",
        "WEIGHT_DECAY = 1e-2\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 15\n",
        "\n",
        "SEQUENCE_LEN = 64\n",
        "CNN_FILTERS = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f939dd5898a0b6",
      "metadata": {
        "id": "b1f939dd5898a0b6"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27110752c038c1d0",
      "metadata": {
        "collapsed": false,
        "id": "27110752c038c1d0"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b050729991fec9",
      "metadata": {
        "collapsed": false,
        "id": "32b050729991fec9"
      },
      "source": [
        "## Load Data & Get Familiar with Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e92f34d72c6f62",
      "metadata": {
        "id": "88e92f34d72c6f62"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "# uploaded = files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B4B36ZV4NgW7",
      "metadata": {
        "id": "B4B36ZV4NgW7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('twitter-suicidal-data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t6IOSCuMUvTj",
      "metadata": {
        "id": "t6IOSCuMUvTj"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pD6_AQ35mWMY",
      "metadata": {
        "id": "pD6_AQ35mWMY"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5EwYfaHAc6dl",
      "metadata": {
        "id": "5EwYfaHAc6dl"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDZUaCB9dA5x",
      "metadata": {
        "id": "bDZUaCB9dA5x"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JZAZwReFloeI",
      "metadata": {
        "id": "JZAZwReFloeI"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XJMKHlv0dHCk",
      "metadata": {
        "id": "XJMKHlv0dHCk"
      },
      "outputs": [],
      "source": [
        "df['intention'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Lvf6FJVDlvL0",
      "metadata": {
        "id": "Lvf6FJVDlvL0"
      },
      "outputs": [],
      "source": [
        "df['intention'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Intention Labels')\n",
        "plt.xlabel('Intention')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f7552c58ea498e",
      "metadata": {
        "collapsed": false,
        "id": "57f7552c58ea498e"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVb32MTIm4hq",
      "metadata": {
        "id": "TVb32MTIm4hq"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HSRWlsBvm_AK",
      "metadata": {
        "id": "HSRWlsBvm_AK"
      },
      "outputs": [],
      "source": [
        "def convert_emoji_to_text(text):\n",
        "    \"\"\"This function would replace emojies with a space\"\"\"\n",
        "    return emoji.demojize(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08VQpZh4nCGd",
      "metadata": {
        "id": "08VQpZh4nCGd"
      },
      "outputs": [],
      "source": [
        "def nltk_pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return 'a'\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return 'v'\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return 'n'\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return 'r'\n",
        "    else:\n",
        "        return 'n'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbe50f445aa09ecc",
      "metadata": {
        "id": "dbe50f445aa09ecc"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(text: str):\n",
        "    \"\"\"\n",
        "    Preprocessing steps are as follows:\n",
        "    1. lowercase the text\n",
        "    2. remove punctuation\n",
        "    3. remove numbers\n",
        "    4. remove urls\n",
        "    5. remove usernames\n",
        "    6. remove extra spaces\n",
        "    7. convert emojis to text\n",
        "    8. remove non-word characters\n",
        "    9. lemmatization and tokenization of the text\n",
        "    10. remove stopwords\n",
        "    :param text: str\n",
        "    :return: tokens: list[str]\n",
        "    \"\"\"\n",
        "\n",
        "    #lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    #remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s@#]', '', text)   ##text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    #remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    #remove urls,\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    #remove usernames\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    #remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    #convert emojis to text\n",
        "    text = convert_emoji_to_text(text)\n",
        "\n",
        "    #remove non-word characters\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    #lemmatization and tokenization of the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    tokens = [lemmatizer.lemmatize(word, nltk_pos_tagger(tag)) for word, tag in pos_tags]\n",
        "\n",
        "    #remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ac7d394e03d9f",
      "metadata": {
        "id": "11ac7d394e03d9f"
      },
      "outputs": [],
      "source": [
        "## TODO: Show some samples before/after preprocessing\n",
        "text =  \"Hi!!   😊  @f102m  https://www.bing.com/ #AI\"\n",
        "preprocess_data(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MrLkfaUYp0Gu",
      "metadata": {
        "id": "MrLkfaUYp0Gu"
      },
      "source": [
        "### 1. Briefly explain the advantages and disadvantages of converting the text to lowercase format and tell why we do this in the processing?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coe2GfJfvXpw",
      "metadata": {
        "id": "coe2GfJfvXpw"
      },
      "source": [
        "#### Advantages:\n",
        "1. **Simplification**:\n",
        "   - **Consistency**: Converting text to lowercase ensures that words like \"Apple\" and \"apple\" are treated as the same word, reducing variability and making the text more uniform.\n",
        "   \n",
        "2. **Improved Model Performance**:\n",
        "   - **Reduced Vocabulary Size**: By treating different capitalizations as the same word, the overall number of unique words (vocabulary size) decreases, which can improve the efficiency and performance of NLP models.\n",
        "\n",
        "3. **Enhanced Accuracy**:\n",
        "   - **Less Noise**: Models can focus on the actual content rather than being distracted by case differences, leading to more accurate text analysis and classification.\n",
        "\n",
        "#### Disadvantages:\n",
        "1. **Loss of Information**:\n",
        "   - **Proper Nouns and Acronyms**: Capitalization can carry important information, such as proper nouns (e.g., \"London\" vs. \"london\") or acronyms (e.g., \"NASA\" vs. \"nasa\"). Lowercasing removes this distinction.\n",
        "\n",
        "2. **Contextual Meaning**:\n",
        "   - **Case-Sensitive Contexts**: In some contexts, the case of letters may change the meaning of words or sentences. For instance, \"US\" (United States) versus \"us\" (pronoun).\n",
        "\n",
        "#### Why We Do This in Processing:\n",
        "- **Uniformity**: Converting text to lowercase creates uniformity, making it easier to process and analyze text data.\n",
        "- **Efficiency**: It reduces the complexity of text data, allowing NLP models to learn patterns more effectively.\n",
        "- **Standard Practice**: It's a common preprocessing step in NLP to ensure that case differences do not affect the analysis, particularly in tasks like sentiment analysis, text classification, and information retrieval.\n",
        "\n",
        "Overall, lowercasing is a trade-off between losing some specific information and gaining a more streamlined, manageable, and often more accurate dataset for NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JQhAQ05Cp8s-",
      "metadata": {
        "id": "JQhAQ05Cp8s-"
      },
      "source": [
        "### 2. Research the elimination of numbers in the above processes and name the advantages and disadvantages of this process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WvSBDf2lwIxG",
      "metadata": {
        "id": "WvSBDf2lwIxG"
      },
      "source": [
        "#### Advantages:\n",
        "\n",
        "1. **Noise Reduction**:\n",
        "   - **Cleaner Data**: Removing numbers can eliminate irrelevant noise, especially if the numbers do not contribute meaningful information to the analysis. This is particularly useful in tasks where numbers are not necessary for understanding the text, such as sentiment analysis or topic modeling.\n",
        "\n",
        "2. **Dimensionality Reduction**:\n",
        "   - **Simplified Models**: By eliminating numbers, the dimensionality of the dataset can be reduced, making it easier for NLP models to process and understand the data. This can lead to more efficient training and improved model performance.\n",
        "\n",
        "3. **Focus on Relevant Features**:\n",
        "   - **Enhanced Feature Extraction**: Removing numbers allows the model to focus on the more relevant textual features without being distracted by numerical data that might not be relevant to the task at hand.\n",
        "\n",
        "#### Disadvantages:\n",
        "\n",
        "1. **Loss of Information**:\n",
        "   - **Quantitative Data Removal**: Numbers can carry important information, such as quantities, dates, or identifiers. Removing them can lead to a loss of valuable context and potentially affect the accuracy of the analysis, especially in tasks where numerical information is critical.\n",
        "\n",
        "2. **Impact on Specific Contexts**:\n",
        "   - **Contextual Relevance**: In some contexts, numbers are essential for understanding the text (e.g., financial reports, scientific data). Removing numbers in such cases can distort the meaning and lead to incorrect interpretations.\n",
        "\n",
        "#### Why We Perform This Processing:\n",
        "\n",
        "Removing numbers is typically done to streamline the text data, reducing noise and focusing the model on the most important textual features. This preprocessing step can be particularly beneficial in general text classification, sentiment analysis, and other NLP tasks where numerical data is not crucial to understanding the content. However, it’s important to consider the specific requirements of the task and the potential value that numerical information might bring to the analysis. Adjusting preprocessing steps to the nature of the text and the goals of the analysis ensures that important information is retained while unnecessary noise is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8uLFStMGqF-G",
      "metadata": {
        "id": "8uLFStMGqF-G"
      },
      "source": [
        "### 3. We have the ability to use hashtags in the Twitter social network. Explain why we did not remove these expressions and what effect does keeping them have on the performance of the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0zbSi157wfgt",
      "metadata": {
        "id": "0zbSi157wfgt"
      },
      "source": [
        "#### Importance of Hashtags\n",
        "\n",
        "1. **Contextual Information**:\n",
        "   - **Semantic Meaning**: Hashtags often contain key information about the topic, sentiment, or context of a tweet. For example, hashtags like `#MentalHealth` or `#Happy` can provide valuable insights into the subject matter or the emotional tone of the tweet.\n",
        "\n",
        "2. **Categorization**:\n",
        "   - **Topic Identification**: Hashtags are used to categorize tweets, making it easier to identify the main themes or topics discussed. This can help in tasks such as topic modeling or clustering, where understanding the main themes is crucial.\n",
        "\n",
        "3. **Trend Analysis**:\n",
        "   - **Popularity and Trends**: Hashtags can indicate trending topics, which are essential for real-time analysis and understanding public opinion. By keeping hashtags, models can better capture these trends and provide more relevant insights.\n",
        "\n",
        "#### Impact on Model Performance\n",
        "\n",
        "1. **Enhanced Feature Set**:\n",
        "   - **Additional Features**: Including hashtags adds to the feature set of the text, providing additional context that can improve the accuracy of text classification models. Hashtags often summarize the key point of the tweet, offering concise and relevant information for the model.\n",
        "\n",
        "2. **Improved Sentiment Analysis**:\n",
        "   - **Sentiment Indicators**: Hashtags can be strong indicators of sentiment. For instance, `#Love` or `#Sad` directly convey emotions that can enhance the performance of sentiment analysis models by providing clear sentiment signals.\n",
        "\n",
        "3. **Contextual Clarity**:\n",
        "   - **Disambiguation**: Hashtags can help disambiguate the meaning of words that might be unclear in isolation. For example, a tweet containing `#Apple` can clarify whether the tweet is about the fruit or the tech company.\n",
        "\n",
        "\n",
        "Keeping hashtags in tweets is beneficial because they carry significant contextual, categorical, and trend-related information. This additional data helps improve the performance of NLP models by providing more features for analysis, enhancing sentiment detection, and aiding in topic identification. Therefore, hashtags are a valuable component in the preprocessing of tweet data and should be preserved to maximize the effectiveness of text-based machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24k5lCShsI17",
      "metadata": {
        "id": "24k5lCShsI17"
      },
      "source": [
        "### 4. Apply the preprocessing function to a few sample tweets and compare before and after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o-gkSebxnrGZ",
      "metadata": {
        "collapsed": true,
        "id": "o-gkSebxnrGZ"
      },
      "outputs": [],
      "source": [
        "df['tokens'] = df['tweet'].apply(preprocess_data)\n",
        "df['token_count'] = df['tokens'].apply(len)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eH7D5kpSshyi",
      "metadata": {
        "id": "eH7D5kpSshyi"
      },
      "source": [
        "### 5.1 Analyze the token lengths of the cleaned tweets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dvTbgMgO9rL",
      "metadata": {
        "id": "8dvTbgMgO9rL"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='intention', y='token_count', data=df)\n",
        "plt.title('Token Length Distribution by Intention Label')\n",
        "plt.xlabel('Intention')\n",
        "plt.ylabel('Token Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yI1SsfvgRlDc",
      "metadata": {
        "id": "yI1SsfvgRlDc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.violinplot(x='intention', y='token_count', data=df)\n",
        "plt.title('Token Length Distribution by Intention Label')\n",
        "plt.xlabel('Intention')\n",
        "plt.ylabel('Token Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OxZpPQCestfy",
      "metadata": {
        "id": "OxZpPQCestfy"
      },
      "source": [
        "### 5.2. Print the statistical summary of token lengths for each intention label and overall dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TA6rsYShu-sD",
      "metadata": {
        "id": "TA6rsYShu-sD"
      },
      "outputs": [],
      "source": [
        "df['token_count'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WBJ6dd3Es0hj",
      "metadata": {
        "id": "WBJ6dd3Es0hj"
      },
      "outputs": [],
      "source": [
        "df.groupby('intention')['token_count'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EJdB_24P-YfZ",
      "metadata": {
        "id": "EJdB_24P-YfZ"
      },
      "source": [
        "### Token Frequency Analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-jgE1R9x-2j3",
      "metadata": {
        "id": "-jgE1R9x-2j3"
      },
      "outputs": [],
      "source": [
        "def warm_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return f\"hsl({random.randint(0, 50)}, 100%, 50%)\"  # red to yellow range\n",
        "\n",
        "def cold_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    return f\"hsl({random.randint(180, 255)}, 100%, 50%)\"  # green to blue range\n",
        "\n",
        "tweets_label_0 = df[df['intention'] == 0]['tokens'].astype(str).str.cat(sep=' ')\n",
        "tweets_label_1 = df[df['intention'] == 1]['tokens'].astype(str).str.cat(sep=' ')\n",
        "\n",
        "wordcloud_0 = WordCloud(width=800, height=400, background_color='white', color_func=cold_color_func).generate(tweets_label_0)\n",
        "wordcloud_1 = WordCloud(width=800, height=400, background_color='white', color_func=warm_color_func).generate(tweets_label_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i0SJKmOH_97A",
      "metadata": {
        "id": "i0SJKmOH_97A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.plot(1, 2, 1)\n",
        "plt.title('Label 0 Word Cloud')\n",
        "plt.imshow(wordcloud_0, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PyIQJbWG__7e",
      "metadata": {
        "id": "PyIQJbWG__7e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(1, 2, 2)\n",
        "plt.title('Label 1 Word Cloud')\n",
        "plt.imshow(wordcloud_1, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e129c7a31663741",
      "metadata": {
        "collapsed": false,
        "id": "2e129c7a31663741"
      },
      "source": [
        "# Word2Vec - Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d15b31459fd1e5",
      "metadata": {
        "id": "8d15b31459fd1e5"
      },
      "outputs": [],
      "source": [
        "# print available word2vec models\n",
        "import gensim.downloader as api\n",
        "print(\"\\n\".join(api.info()['models'].keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55bbe6faa1226b8d",
      "metadata": {
        "id": "55bbe6faa1226b8d"
      },
      "outputs": [],
      "source": [
        "W2V_PATH = None # Path to W2V if downloaded\n",
        "if W2V_PATH is not None and os.path.exists(W2V_PATH):\n",
        "    print(\"Loading Word2Vec model...\")\n",
        "    w2v_model = gensim.models.KeyedVectors.load(W2V_PATH, mmap='r')\n",
        "    print(\"Word2Vec model is loaded.\")\n",
        "else:\n",
        "    print(\"Downloading Word2Vec model...\")\n",
        "    w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "    print(\"Word2vec model is downloaded.\")\n",
        "    if W2V_PATH is not None:\n",
        "      print(\"\\nSaving Word2Vec model...\")\n",
        "      w2v_model.save(W2V_PATH)\n",
        "      print(\"Word2Vec model is saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d53926889defc38",
      "metadata": {
        "id": "7d53926889defc38"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_VECTOR_DIM = w2v_model.vector_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21730f8dd01e5b4e",
      "metadata": {
        "collapsed": false,
        "id": "21730f8dd01e5b4e"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xdqLYrA4xjuX",
      "metadata": {
        "id": "xdqLYrA4xjuX"
      },
      "outputs": [],
      "source": [
        "class Twitter(Dataset):\n",
        "    def __init__(self, dataframe: pd.DataFrame, w2v_model: gensim.models.KeyedVectors, sequence_len: int):\n",
        "        self.dataframe = dataframe.copy()\n",
        "        self.w2v_model = w2v_model\n",
        "        self.max_sequence_len = sequence_len\n",
        "        self.vector_size = w2v_model.vector_size\n",
        "\n",
        "        self.df_token_col = \"tokens\"\n",
        "\n",
        "\n",
        "        self._proc_dataset()\n",
        "\n",
        "        self.len = len(self.dataframe)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.dataframe.iloc[idx][\"vector\"], self.dataframe.iloc[idx][\"intention\"]\n",
        "\n",
        "    def get_vector_size(self):\n",
        "        return self.vector_size\n",
        "\n",
        "    def _proc_dataset(self):\n",
        "        # Preprocess and return tokens list\n",
        "        self.dataframe[self.df_token_col] = self.dataframe[\"tweet\"].map(preprocess_data)\n",
        "\n",
        "        # delete samples with empty tokens\n",
        "        lwz = len(self.dataframe)\n",
        "        self.dataframe = self.dataframe[self.dataframe[self.df_token_col].map(len) > 0]\n",
        "        self.dataframe.reset_index(drop=True, inplace=True)\n",
        "        print(f\"Deleted 0-Len Samples: {lwz - len(self.dataframe)}\")\n",
        "        print(self.dataframe[self.df_token_col])\n",
        "        # Add padding\n",
        "        self.dataframe[self.df_token_col] = self.dataframe[self.df_token_col].map(self._pad)\n",
        "\n",
        "        # Get embedding's vectors\n",
        "        self.dataframe[\"vector\"] = self.dataframe[self.df_token_col].map(self._get_word_vectors)\n",
        "\n",
        "    def _get_word_vectors(self, tokens: list) -> torch.Tensor:\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in self.w2v_model:\n",
        "                vectors.append(self.w2v_model[token])\n",
        "            else:\n",
        "                vectors.append(np.zeros(self.vector_size))\n",
        "        return torch.tensor(vectors, dtype=torch.float32)\n",
        "\n",
        "\n",
        "    def _pad(self, tokens: list):\n",
        "        if len(tokens) < self.max_sequence_len:\n",
        "            padding = [''] * (self.max_sequence_len - len(tokens))\n",
        "            padded_tokens = tokens + padding\n",
        "        else:\n",
        "            padded_tokens = tokens[:self.max_sequence_len]\n",
        "        return padded_tokens\n",
        "\n",
        "    def seq_report(self):\n",
        "        length_all = self.dataframe[self.df_token_col].map(len).tolist()\n",
        "        max_length = np.max(length_all)\n",
        "        print(f\"Sequence Length Report\")\n",
        "        print(f\":::::MAX  LENGTH:::[{max_length:^5}]\")\n",
        "        print(f\":::::MIN  LENGTH:::[{np.min(length_all):^5}]\")\n",
        "        print(f\":::::MEAN LENGTH:::[{np.mean(length_all):^5}]\")\n",
        "\n",
        "        all_tokens = set()\n",
        "        for token_set in self.dataframe[self.df_token_col].tolist():\n",
        "            all_tokens = all_tokens.union(set(token_set))\n",
        "        unique_tokens_count = len(all_tokens)\n",
        "        valid_tokens = sum(1 if token in self.w2v_model else 0 for token in all_tokens)\n",
        "        print(\"Sequence Tokenization Report\")\n",
        "        print(f\":::::All Unique Tokens:::[{unique_tokens_count:^6}\")\n",
        "        print(f\":::::All Valid Tokens:::[{valid_tokens:^6}\")\n",
        "        print(f\":::::Valid Tokens:::[{round(100*valid_tokens/unique_tokens_count, 2):^5}%]\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_tensor(tokens: list):\n",
        "        return torch.tensor(tokens, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47xifZ1xeWs",
      "metadata": {
        "id": "a47xifZ1xeWs"
      },
      "outputs": [],
      "source": [
        "dataset = Twitter(df, w2v_model, SEQUENCE_LEN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vvEInQq7yiCf",
      "metadata": {
        "id": "vvEInQq7yiCf"
      },
      "outputs": [],
      "source": [
        "dataset.seq_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PB6Pbrn5xfXB",
      "metadata": {
        "id": "PB6Pbrn5xfXB"
      },
      "source": [
        "### 6. Explain what are the methods of dealing with non-existent words in the mentioned dictionary and name the advantages and disadvantages of each."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qQM1seZsxiwD",
      "metadata": {
        "id": "qQM1seZsxiwD"
      },
      "source": [
        "When dealing with non-existent words in a dictionary (such as a word embedding dictionary like Word2Vec or GloVe), several methods can be employed to handle these out-of-vocabulary (OOV) words. Each method has its advantages and disadvantages. Here are some common strategies:\n",
        "\n",
        "#### 1. Using a Default Vector\n",
        "**Method:**\n",
        "- Assign a default vector (e.g., a zero vector or a random vector) to any OOV word.\n",
        "\n",
        "**Advantages:**\n",
        "- **Simplicity:** Easy to implement and requires minimal additional processing.\n",
        "- **Consistency:** Ensures that every word has a representation, avoiding issues with missing data.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Loss of Information:** Default vectors do not capture any semantic information about the OOV words, which can reduce the model's accuracy.\n",
        "- **Homogeneity:** Treats all OOV words the same, regardless of their potential different meanings or contexts.\n",
        "\n",
        "#### 2. Using Subword Information (Character-level Models)\n",
        "**Method:**\n",
        "- Break down OOV words into smaller subword units (e.g., characters or n-grams) and create embeddings based on these subword units.\n",
        "\n",
        "**Advantages:**\n",
        "- **Better Generalization:** Can provide meaningful representations for OOV words based on their subword components.\n",
        "- **Robustness to Typos and Morphology:** Can handle variations of words, such as typos or different morphological forms (e.g., plurals, conjugations).\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Increased Complexity:** More complex to implement and computationally intensive.\n",
        "- **Resource Intensive:** Requires additional resources for training and inference.\n",
        "\n",
        "#### 3. Retraining the Model with New Vocabulary\n",
        "**Method:**\n",
        "- Periodically retrain the word embedding model with updated corpora that include the new vocabulary.\n",
        "\n",
        "**Advantages:**\n",
        "- **Up-to-date Embeddings:** Ensures that the model is continuously updated with the latest vocabulary and language use.\n",
        "- **Enhanced Representations:** Provides better and more accurate representations for the entire vocabulary, including previously OOV words.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Time-Consuming:** Retraining can be computationally expensive and time-consuming.\n",
        "- **Requires Continuous Data Collection:** Needs a constant supply of new data to update the vocabulary.\n",
        "\n",
        "#### 4. Contextual Embeddings (e.g., BERT, ELMo)\n",
        "**Method:**\n",
        "- Use models like BERT or ELMo, which generate word embeddings based on the context in which a word appears rather than relying on a fixed dictionary.\n",
        "\n",
        "**Advantages:**\n",
        "- **Dynamic Representations:** Generates different embeddings for a word based on its context, handling polysemy (multiple meanings) effectively.\n",
        "- **Handles OOV Words:** Better equipped to handle OOV words by leveraging the surrounding context.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **High Computational Cost:** Requires significant computational resources for training and inference.\n",
        "- **Complexity:** More complex architecture and implementation compared to static embeddings.\n",
        "\n",
        "#### 5. Lookup in External Resources\n",
        "**Method:**\n",
        "- Use external resources like WordNet, dictionaries, or thesauruses to find synonyms or similar words for the OOV word and use their embeddings.\n",
        "\n",
        "**Advantages:**\n",
        "- **Enriches Vocabulary:** Can expand the vocabulary and improve embeddings by leveraging external knowledge.\n",
        "- **Contextual Similarity:** Provides embeddings that are semantically similar to the OOV word.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Dependency on External Resources:** Requires access to and integration with external resources, which may not always be up-to-date or comprehensive.\n",
        "- **Potential Inconsistencies:** The quality and consistency of external resources can vary, leading to potential issues in embedding quality.\n",
        "\n",
        "#### 6. Using Contextual Averaging\n",
        "**Method:**\n",
        "- Average the embeddings of the surrounding words to generate a proxy embedding for the OOV word based on its context.\n",
        "\n",
        "**Advantages:**\n",
        "- **Context-Aware:** Provides a context-specific embedding for the OOV word, capturing some semantic information from its surroundings.\n",
        "- **Simple Implementation:** Relatively straightforward to implement compared to more complex models.\n",
        "\n",
        "**Disadvantages:**\n",
        "- **Approximation:** The averaged vector is an approximation and may not fully capture the meaning of the OOV word.\n",
        "- **Context Dependency:** Performance heavily depends on the quality and relevance of the surrounding context.\n",
        "\n",
        "Each method has its trade-offs between simplicity, computational cost, and the richness of the resulting word representations. The choice of method depends on the specific requirements and constraints of the application at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b87e7e8b267a1c8",
      "metadata": {
        "collapsed": false,
        "id": "4b87e7e8b267a1c8"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13h5Re2ATU78",
      "metadata": {
        "id": "13h5Re2ATU78"
      },
      "source": [
        "### 7. Briefly explain how the Adam optimizer works and how it differs from the SGD optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QRKFqGLSTbx-",
      "metadata": {
        "id": "QRKFqGLSTbx-"
      },
      "source": [
        "#### Adam Optimizer\n",
        "\n",
        "The Adam optimizer is a popular optimization algorithm used for training deep neural networks. It combines the benefits of two other methods: AdaGrad and RMSProp. Here's a simple explanation of how it works:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - Adam starts by initializing two vectors to keep track of the moving averages of the gradients.\n",
        "\n",
        "2. **Gradient Calculation**:\n",
        "   - For each training step, Adam calculates the gradient of the loss function with respect to the model parameters.\n",
        "\n",
        "3. **Update Moving Averages**:\n",
        "   - Adam updates two moving averages:\n",
        "     - The first moving average estimates the mean of the gradients.\n",
        "     - The second moving average estimates the uncentered variance of the gradients.\n",
        "\n",
        "4. **Bias Correction**:\n",
        "   - Adam adjusts these moving averages to correct for their bias towards zero, especially in the initial training steps.\n",
        "\n",
        "5. **Parameter Update**:\n",
        "   - The model parameters are then updated using these corrected moving averages. This helps in achieving a more stable and efficient convergence.\n",
        "\n",
        "#### Differences Between Adam and SGD\n",
        "\n",
        "##### Stochastic Gradient Descent (SGD)\n",
        "\n",
        "- **Learning Rate**:\n",
        "  - SGD uses a fixed learning rate for all parameters. This learning rate needs to be carefully tuned and can significantly affect the training process.\n",
        "\n",
        "- **Gradient Descent**:\n",
        "  - SGD updates the model parameters by moving them in the direction of the negative gradient. This direction is determined by the current batch of training data.\n",
        "\n",
        "- **Simplicity**:\n",
        "  - SGD is simple and easy to implement but can struggle with noisy data and may converge slowly.\n",
        "\n",
        "###### Adam\n",
        "\n",
        "- **Adaptive Learning Rates**:\n",
        "  - Adam adjusts the learning rate for each parameter individually. This means it can adapt the learning rate based on how the gradients change, leading to more efficient training.\n",
        "\n",
        "- **Combination of Momentum and RMSProp**:\n",
        "  - Adam uses concepts from both momentum (to smooth out the updates) and RMSProp (to adapt the learning rates), making it robust to noisy gradients and effective for a wide range of problems.\n",
        "\n",
        "- **Bias Correction**:\n",
        "  - Adam includes a mechanism to correct for biases in the estimates of the moving averages, especially at the start of training.\n",
        "\n",
        "##### Summary\n",
        "\n",
        "- **SGD** is straightforward and works well for many problems, but it requires careful tuning of the learning rate and may struggle with noisy gradients.\n",
        "- **Adam** provides a more adaptive approach, automatically adjusting learning rates and combining the strengths of momentum and RMSProp, making it well-suited for complex and noisy problems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bvNy5ErLlnrq",
      "metadata": {
        "id": "bvNy5ErLlnrq"
      },
      "source": [
        "### 8. Use the Cross Entropy cost function. Also, tell the reason for using this cost function according to the nature of the problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uyNo2HXBuMur",
      "metadata": {
        "id": "uyNo2HXBuMur"
      },
      "source": [
        "#### Cross Entropy Cost Function\n",
        "\n",
        "The Cross Entropy cost function, often used in classification problems, measures the performance of a classification model whose output is a probability value between 0 and 1. Here's a brief explanation of why it is used and how it aligns with the nature of classification problems:\n",
        "\n",
        "#### What is Cross Entropy?\n",
        "\n",
        "Cross Entropy is a measure of the difference between two probability distributions for a given set of events. In the context of neural networks, it is used to measure the performance of a classification model by comparing the predicted probability distribution (outputs) with the true distribution (actual labels).\n",
        "\n",
        "#### Why Use Cross Entropy for Classification?\n",
        "\n",
        "1. **Probabilistic Interpretation**:\n",
        "   - Cross Entropy cost function directly aligns with the probabilistic interpretation of the model’s output. It measures how well the predicted probabilities match the true class labels.\n",
        "\n",
        "2. **Gradient Behavior**:\n",
        "   - The Cross Entropy cost function has a smooth gradient, which helps in efficient learning and convergence. It provides a strong signal when the prediction is far from the true label and a smaller signal as it gets closer, leading to more stable and faster convergence.\n",
        "\n",
        "3. **Penalizing Incorrect Predictions**:\n",
        "   - Cross Entropy penalizes incorrect predictions more heavily than correct ones. This characteristic helps the model to focus more on reducing the probability assigned to incorrect classes and increasing the probability of the correct class.\n",
        "\n",
        "4. **Class Imbalance**:\n",
        "   - For imbalanced datasets, Cross Entropy helps the model to still learn effectively. It ensures that the model is penalized according to the actual probability of the class rather than the absolute count of misclassifications.\n",
        "\n",
        "#### Applicability to the Problem\n",
        "\n",
        "In classification problems, especially those involving multiple classes (multi-class classification), the objective is to assign the correct class label to each input sample. The output of the model is often a probability distribution over all possible classes. Cross Entropy is particularly suitable for these scenarios because:\n",
        "\n",
        "- **Alignment with Output**: The nature of classification problems involves predicting class probabilities. Cross Entropy directly measures the deviation between predicted probabilities and actual classes.\n",
        "- **Effective Learning**: By penalizing incorrect predictions more heavily, it drives the model to focus on getting the probability distribution as close as possible to the true distribution, improving the model’s accuracy.\n",
        "\n",
        "#### Summary\n",
        "\n",
        "Using the Cross Entropy cost function in classification problems is effective because it aligns well with the goal of minimizing the difference between predicted and actual probability distributions. It provides an intuitive and mathematically sound way to measure the performance of a classification model, encouraging the model to produce outputs that are closer to the actual class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd00ab880892e646",
      "metadata": {
        "collapsed": false,
        "id": "bd00ab880892e646"
      },
      "source": [
        "## Split Data into train-valid"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iPxXZPGxvbF5",
      "metadata": {
        "id": "iPxXZPGxvbF5"
      },
      "source": [
        "### 9.   Now divide the data into two parts, training and testing. Report the ratio of this division and why you use this ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WJfGGSgBveBD",
      "metadata": {
        "id": "WJfGGSgBveBD"
      },
      "source": [
        "In machine learning, it is crucial to divide your dataset into training and testing sets to evaluate the performance of your model on unseen data. A common practice is to use a specific ratio for this division.\n",
        "\n",
        "#### Common Ratios\n",
        "\n",
        "- **80/20 Split**: 80% of the data is used for training and 20% for testing.\n",
        "- **70/30 Split**: 70% of the data is used for training and 30% for testing.\n",
        "- **75/25 Split**: 75% of the data is used for training and 25% for testing.\n",
        "\n",
        "#### Recommended Ratio\n",
        "\n",
        "The most common and recommended ratio is the **80/20 split**. This ratio is widely used because it provides a good balance between having enough data to train the model and enough data to test the model's performance.\n",
        "\n",
        "#### Why Use the 80/20 Ratio?\n",
        "\n",
        "1. **Sufficient Training Data**:\n",
        "   - Using 80% of the data for training ensures that the model has a substantial amount of data to learn from. This is especially important for deep learning models that require large amounts of data to perform well.\n",
        "\n",
        "2. **Adequate Testing Data**:\n",
        "   - Reserving 20% of the data for testing ensures that there is a significant portion of the data that the model has not seen during training. This helps in effectively evaluating the model's performance and generalization capability.\n",
        "\n",
        "3. **Balance Between Bias and Variance**:\n",
        "   - The 80/20 split helps strike a balance between bias and variance. It ensures that the training set is large enough to minimize bias and the testing set is large enough to give an accurate estimate of the model’s variance.\n",
        "\n",
        "4. **Standard Practice**:\n",
        "   - The 80/20 split is a standard practice in the machine learning community. It allows for consistent comparison of results across different studies and projects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a54facda2729872",
      "metadata": {
        "id": "6a54facda2729872"
      },
      "outputs": [],
      "source": [
        "train_df, valid_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670fcd8db0e5651c",
      "metadata": {
        "collapsed": false,
        "id": "670fcd8db0e5651c"
      },
      "source": [
        "## Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e4a5b690afadad5",
      "metadata": {
        "id": "3e4a5b690afadad5"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "train_dataset = Twitter(train_df, w2v_model, sequence_len=SEQUENCE_LEN)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len=SEQUENCE_LEN)\n",
        "\n",
        "print(f\"Train dataset length: {len(train_dataset)}\")\n",
        "print(f\"Valid dataset length: {len(valid_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "racbKRMLn7HY",
      "metadata": {
        "id": "racbKRMLn7HY"
      },
      "outputs": [],
      "source": [
        "train_dataset.seq_report()\n",
        "print(\"_______________________\")\n",
        "valid_dataset.seq_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd1da733e79676ff",
      "metadata": {
        "collapsed": false,
        "id": "fd1da733e79676ff"
      },
      "source": [
        "# Model and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa29012e257e3d9",
      "metadata": {
        "collapsed": false,
        "id": "aa29012e257e3d9"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b33657ae501516",
      "metadata": {
        "id": "d5b33657ae501516"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def model_eval(model, loader, loss_function, device = 'auto'):\n",
        "    \"\"\"Returns test_loss, test_acc\"\"\"\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    if device == \"auto\":\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(device)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    itr = tqdm(loader, total=len(loader), leave=False)\n",
        "\n",
        "    for inputs, labels in itr:\n",
        "        # Move inputs and labels to the appropriate device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # use model's forward pass to generate outputs\n",
        "        outputs = model(inputs.transpose(1, 2))\n",
        "\n",
        "        # calculate model's loss\n",
        "        loss = loss_function(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # calculate/update model's accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        itr.set_description(\"(Eval)\")\n",
        "        itr.set_postfix(\n",
        "            loss=round(loss.item(), 5),\n",
        "            accuracy=round(correct_predictions / total_predictions, 5),\n",
        "        )\n",
        "\n",
        "    test_loss = test_loss / len(loader)\n",
        "    test_acc = correct_predictions / total_predictions\n",
        "\n",
        "    return test_loss, test_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bc1edee98f616b5",
      "metadata": {
        "id": "1bc1edee98f616b5"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "        model,\n",
        "        batch_size,\n",
        "        loss_function,\n",
        "        optimizer,\n",
        "        epochs,\n",
        "        train_set,\n",
        "        valid_set,\n",
        "        device: str = 'auto'\n",
        "):\n",
        "\n",
        "    if device == \"auto\":\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    else:\n",
        "        device = torch.device(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "\n",
        "    # create dataloaders from datasets\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        itr = tqdm(train_loader, total=len(train_loader), leave=False)\n",
        "\n",
        "        for idx, (inputs, labels) in enumerate(itr, start=1):\n",
        "            # move model's inputs to `device`\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # use model's forward pass to generate outputs\n",
        "            outputs = model(inputs.transpose(1, 2))\n",
        "\n",
        "            # process model's predictipns and calculate/update accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            epoch_acc = correct / total\n",
        "\n",
        "            # calculate model's loss and update epoch's loss\n",
        "            loss = loss_function(outputs, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # clear optimizer's state and zero previous grads\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # backward calculated loss\n",
        "            loss.backward()\n",
        "\n",
        "            # step optimizer\n",
        "            optimizer.step()\n",
        "\n",
        "            itr.set_description(f\"(Training) Epoch [{epoch + 1}/{epochs}]\")\n",
        "            itr.set_postfix(\n",
        "                loss=round(loss.item(), 5),\n",
        "                accuracy=round(epoch_acc, 5),\n",
        "            )\n",
        "\n",
        "        epoch_loss = (epoch_loss / len(train_loader) )\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss, valid_acc = model_eval(\n",
        "            model=model,\n",
        "            loader=valid_loader,\n",
        "            loss_function=loss_function,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accs.append(valid_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} :\")\n",
        "        print(f\"train_loss = {epoch_loss :.6f}, train_acc = {epoch_acc:.6f}, valid_loss =  {valid_loss:.6f}, valid_acc = {valid_acc:.6f}\")\n",
        "        print(40 * \"_\")\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": train_losses,\n",
        "        \"train_acc\": train_accs,\n",
        "        \"valid_loss\": valid_losses,\n",
        "        \"valid_acc\": valid_accs,\n",
        "    }\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5acc20a7d5d446",
      "metadata": {
        "id": "6e5acc20a7d5d446"
      },
      "outputs": [],
      "source": [
        "def trend_plot_helper(pobj):\n",
        "    plt.figure(figsize=(5*len(pobj), 5))\n",
        "    for idx, (titler, plots) in enumerate(pobj.items(), start=1):\n",
        "        plt.subplot(1, len(pobj), idx)\n",
        "        for label, trend in plots:\n",
        "            plt.plot(range(1, len(trend)+1), trend, label=label)\n",
        "        yt, xt = titler.split(' - ')\n",
        "        plt.xlabel(xt)\n",
        "        plt.ylabel(yt)\n",
        "        plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "819f88c36492de48",
      "metadata": {
        "id": "819f88c36492de48"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_confusion_matrix(model, dataset, device='auto'):\n",
        "    if device == 'auto':\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    itr = tqdm(loader, leave=False, desc=\"Generate data\")\n",
        "\n",
        "    labels = []\n",
        "    predicted = []\n",
        "\n",
        "    for inputs, targets in itr:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        outputs = model(inputs.transpose(1, 2))\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        labels.extend(targets.cpu().numpy())\n",
        "        predicted.extend(preds.cpu().numpy())\n",
        "\n",
        "    labels = np.array(labels)\n",
        "    predicted = np.array(predicted)\n",
        "\n",
        "    cm = metrics.confusion_matrix(\n",
        "        y_true=labels,\n",
        "        y_pred=predicted,\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, cmap='Blues', annot=True, cbar=False, fmt=\".0f\")\n",
        "    plt.xlabel('Predicted Label', labelpad=20)\n",
        "    plt.ylabel('True Label', labelpad=20)\n",
        "    plt.title('Confusion Matrix', fontsize=30)\n",
        "\n",
        "    recall = metrics.recall_score(y_true=labels, y_pred=predicted, average='macro')\n",
        "    f1 = metrics.f1_score(y_true=labels, y_pred=predicted, average='macro')\n",
        "    precision = metrics.precision_score(y_true=labels, y_pred=predicted, average='macro')\n",
        "    report = metrics.classification_report(y_true=labels, y_pred=predicted)\n",
        "\n",
        "    return {'recall': recall, 'f1': f1, 'precision': precision, 'report': report}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1566b2a8428318c8",
      "metadata": {
        "collapsed": false,
        "id": "1566b2a8428318c8"
      },
      "source": [
        "## Model's Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2ed27ccf8d0307c",
      "metadata": {
        "id": "a2ed27ccf8d0307c"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size = 300, sequence_len_1_ = 64, sequence_len_2_ = 128, output_size = 2, flatt_size = 12288):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1_1 = nn.Conv1d(input_size, sequence_len_1, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv1d(input_size, sequence_len_1, kernel_size=5, padding=2)\n",
        "        self.conv1_3 = nn.Conv1d(input_size, sequence_len_1, kernel_size=7, padding=3)\n",
        "\n",
        "        self.conv2_1 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=5, padding=2)\n",
        "        self.conv2_3 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=7, padding=3)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(flatt_size, sequence_len_2)\n",
        "        self.fc2 = nn.Linear(sequence_len_2, output_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.relu(self.conv1_1(x))\n",
        "        x2 = self.relu(self.conv1_2(x))\n",
        "        x3 = self.relu(self.conv1_3(x))\n",
        "\n",
        "        x1 = self.pool(self.relu(self.conv2_1(x1)))\n",
        "        x2 = self.pool(self.relu(self.conv2_2(x2)))\n",
        "        x3 = self.pool(self.relu(self.conv2_3(x3)))\n",
        "\n",
        "        x_concatenated = torch.cat((x1, x2, x3), dim=1)\n",
        "\n",
        "        x = x_concatenated.view(x_concatenated.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yt9SMAxCv4ak",
      "metadata": {
        "id": "Yt9SMAxCv4ak"
      },
      "source": [
        "### 10. What is the effect of kernel size in convolution layers and how is it effective in extracting input features? What does it mean to be more or less?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zu2u3POZv8J1",
      "metadata": {
        "id": "Zu2u3POZv8J1"
      },
      "source": [
        "In convolutional neural networks (CNNs), the kernel size (or filter size) in convolution layers is a critical hyperparameter that directly impacts how the network extracts features from the input data. The kernel size determines the dimensions of the filter that slides over the input to produce feature maps.\n",
        "\n",
        "##### Effects of Kernel Size\n",
        "\n",
        "1. **Small Kernel Size *:*\n",
        "   - **Fine Details**: Small kernels capture fine-grained details and local patterns in the input data.\n",
        "   - **Less Computationally Expensive**: Smaller filters require fewer computations, making the network faster and less resource-intensive.\n",
        "   - **Stacking Layers**: Using smaller kernels allows stacking more convolutional layers, enabling the network to learn hierarchical features..\n",
        "\n",
        "2. **Large Kernel Size **:\n",
        "   - **Broader Context**: Larger kernels capture more global patterns and broader context from the input data.\n",
        "   - **More Computationally Expensive**: Larger filters increase the computational load and memory usage.\n",
        "   - **Potential Overfitting**: With fewer layers, larger kernels might lead to overfitting as they capture more complex patterns directly.\n",
        "\n",
        "#### Choosing Kernel Size\n",
        "\n",
        "- **Nature of the Data**:\n",
        "  - For images with fine details (e.g., textures, edges), smaller kernels are effective.\n",
        "  - For images requiring broader context (e.g., large objects or patterns), larger kernels might be beneficial.\n",
        "\n",
        "- **Network Architecture**:\n",
        "  - Modern architectures often use small kernels and increase the network's depth to capture hierarchical features.\n",
        "  - In some architectures, a combination of small and large kernels is used to balance local and global feature extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8cONKXFzdJ4",
      "metadata": {
        "id": "d8cONKXFzdJ4"
      },
      "source": [
        "### 11. In your opinion, why didn't we reduce the convolution output and did this reduction through Feed Forward layers and what advantages can this layer have over alternative methods? Investigate the reason for this and state the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sInGmAklzgD8",
      "metadata": {
        "id": "sInGmAklzgD8"
      },
      "source": [
        "#### Reasons for Not Reducing Convolution Output Explicitly:\n",
        "\n",
        "1. **Preserving Spatial Information**:\n",
        "   - Convolutional layers are designed to capture spatial hierarchies and local patterns in the input data, such as edges, textures, and shapes in images. Reducing the dimensionality too early could discard valuable spatial information that is crucial for understanding the input, especially in the early stages of a CNN.\n",
        "\n",
        "2. **Learning Rich Feature Maps**:\n",
        "   - Convolutional layers progressively learn more abstract and complex features as the data passes through the network. By maintaining the full dimensionality, the model can capture a richer set of features, which can be crucial for achieving high performance in tasks like image recognition, where detailed information at different scales is important.\n",
        "\n",
        "3. **Flexibility for Fully Connected Layers**:\n",
        "   - Fully connected (FC) layers offer flexibility for dimensionality reduction later in the network architecture. This allows for more control over the final feature representation before classification, enabling the model to make more informed and accurate predictions.\n",
        "\n",
        "#### Advantages of Fully Connected (FC) Layers for Dimensionality Reduction:\n",
        "\n",
        "1. **Learned Reduction**:\n",
        "   - FC layers can learn the optimal way to reduce the dimensionality of the features based on the specific task at hand. This learned reduction is often more effective than pre-defined methods such as pooling layers (e.g., max pooling, average pooling) because it can adapt to the unique characteristics of the dataset.\n",
        "\n",
        "2. **Non-linear Activation**:\n",
        "   - FC layers can be combined with non-linear activation functions (e.g., ReLU) to introduce non-linearity before dimensionality reduction. This allows the model to learn complex feature representations that simple pooling operations might not capture, enhancing the model’s ability to differentiate between classes.\n",
        "\n",
        "3. **Adaptability**:\n",
        "   - FC layers provide greater control over the final feature representation. Researchers and practitioners can experiment with different numbers of neurons in the FC layers to achieve the desired level of dimensionality reduction while maintaining good performance. This adaptability is particularly useful when fine-tuning the model for specific tasks or datasets.\n",
        "\n",
        "#### Investigation and Results\n",
        "\n",
        "To investigate the impact of not reducing the convolution output early and relying on FC layers for dimensionality reduction, we can look at the performance and architecture of several successful CNN models:\n",
        "\n",
        "1. **VGG Network**:\n",
        "   - In the VGG architecture, a series of convolutional layers are used to extract features, followed by three fully connected layers for the final classification. The convolutional layers maintain spatial dimensions until the final layers, ensuring rich feature extraction. The FC layers then reduce the dimensionality for the classification task, contributing to the model’s high accuracy on image recognition tasks.\n",
        "\n",
        "2. **ResNet (Residual Networks)**:\n",
        "   - ResNet uses skip connections to create very deep networks while avoiding the vanishing gradient problem. The convolutional layers are responsible for feature extraction, and the final few layers are fully connected layers that reduce the dimensionality for classification. This architecture has achieved state-of-the-art performance on various benchmarks by leveraging the strengths of both convolutional and fully connected layers.\n",
        "\n",
        "3. **Inception Networks**:\n",
        "   - Inception networks use multiple kernel sizes in parallel to capture features at different scales. The output from these convolutional layers is concatenated and then passed through fully connected layers for dimensionality reduction and classification. This approach demonstrates the effectiveness of maintaining convolutional output dimensions until the final layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-XhOZ_qwF6Bh",
      "metadata": {
        "id": "-XhOZ_qwF6Bh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RAlDfhIh78Z-",
      "metadata": {
        "id": "RAlDfhIh78Z-"
      },
      "source": [
        "### 12. How do different batch sizes and learning rates affect the training performance and generalization of a Convolutional Neural Network (CNN) model on our specific dataset, and what are the optimal values for these hyperparameters based on our observed training and validation metrics?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ju20u-AL8AVl",
      "metadata": {
        "id": "ju20u-AL8AVl"
      },
      "source": [
        "#### Effects of Batch Size\n",
        "\n",
        "1. **Smaller Batch Size**:\n",
        "   - **Noisy Gradient Estimates**: Smaller batch sizes produce noisier gradient estimates. This noise can help in escaping local minima, potentially leading to better generalization, but can also cause more fluctuation in the loss during training.\n",
        "   - **Faster Updates**: Each epoch processes more updates, which can lead to faster convergence in terms of epochs but might require more overall computation.\n",
        "   - **Memory Efficiency**: Smaller batches require less memory, making it possible to train larger models on hardware with limited memory.\n",
        "\n",
        "2. **Larger Batch Size**:\n",
        "   - **Smoother Gradient Estimates**: Larger batches provide more accurate estimates of the gradient, leading to smoother convergence curves. However, this might also cause the model to get stuck in sharp local minima.\n",
        "   - **Slower Updates**: Each epoch processes fewer updates, which can slow down the convergence in terms of epochs but often requires fewer epochs to converge.\n",
        "   - **Better Parallelism**: Larger batches can take better advantage of modern hardware (like GPUs), potentially speeding up the wall-clock time per epoch.\n",
        "\n",
        "#### Effects of Learning Rate\n",
        "\n",
        "1. **Lower Learning Rate**:\n",
        "   - **Slow Convergence**: Smaller learning rates lead to more gradual updates of the model parameters, often resulting in slower convergence. This can be beneficial for fine-tuning and avoiding overshooting minima.\n",
        "   - **More Precise**: Can achieve more precise minima, but if too low, might get stuck in a local minima or take an impractically long time to converge.\n",
        "\n",
        "2. **Higher Learning Rate**:\n",
        "   - **Faster Convergence**: Larger learning rates can accelerate convergence, especially in the initial stages. However, they might cause the model to overshoot minima or diverge if too high.\n",
        "   - **Potential Instability**: Too high learning rates can lead to unstable training, with the loss fluctuating or even increasing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G1sGzKGY-S2F",
      "metadata": {
        "id": "G1sGzKGY-S2F"
      },
      "source": [
        "### 1:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6vAGD-qm7lu1",
      "metadata": {
        "id": "6vAGD-qm7lu1"
      },
      "outputs": [],
      "source": [
        "input_size = 300\n",
        "output_size = 2\n",
        "sequence_len_1 = 64\n",
        "sequence_len_2 = 128\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE_1 = 0.0004\n",
        "BATCH_SIZE_1 = 64\n",
        "LEARNING_RATE_2 = 0.0004\n",
        "BATCH_SIZE_2 = 128\n",
        "LEARNING_RATE_3 = 0.001\n",
        "BATCH_SIZE_3 = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-3uG12tU7lu8",
      "metadata": {
        "id": "-3uG12tU7lu8"
      },
      "source": [
        "#### 1:\n",
        "LEARNING_RATE_1 = 0.0004   \n",
        "BATCH_SIZE_1 = 64  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l3QOwsdX7lu8",
      "metadata": {
        "id": "l3QOwsdX7lu8"
      },
      "outputs": [],
      "source": [
        "train_dataset = Twitter(train_df, w2v_model, sequence_len_1)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RB0l4h5S7lu8",
      "metadata": {
        "id": "RB0l4h5S7lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN(input_size,sequence_len_1, sequence_len_2, output_size, int(3 * sequence_len_1 * sequence_len_2 * 0.5))\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_jOkbeF7lu8",
      "metadata": {
        "id": "E_jOkbeF7lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model_train_history = train_model(\n",
        "    model=cnn_model,\n",
        "    batch_size=BATCH_SIZE_1,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    epochs=EPOCHS,\n",
        "    train_set=train_dataset,\n",
        "    valid_set=valid_dataset,\n",
        "    device='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SGXkDhh37lu8",
      "metadata": {
        "id": "SGXkDhh37lu8"
      },
      "outputs": [],
      "source": [
        "trend_plot_helper(\n",
        "    {\n",
        "        \"Accuracy - Epoch\": [\n",
        "            (\"Train Acc\", cnn_model_train_history[\"train_acc\"]),\n",
        "            (\"Validation Acc\", cnn_model_train_history[\"valid_acc\"]),\n",
        "        ],\n",
        "        \"Loss - Epoch\": [\n",
        "            (\"Train Loss\", cnn_model_train_history[\"train_loss\"]),\n",
        "            (\"Validation Loss\", cnn_model_train_history[\"valid_loss\"])\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DYKvoog17lu8",
      "metadata": {
        "id": "DYKvoog17lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model_report = generate_confusion_matrix(\n",
        "    model=cnn_model,\n",
        "    dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W19etXAq7lu8",
      "metadata": {
        "id": "W19etXAq7lu8"
      },
      "outputs": [],
      "source": [
        "print(f\"Recall:    {cnn_model_report['recall']:.3f}\")\n",
        "print(f\"F1:        {cnn_model_report['f1']:.3f}\")\n",
        "print(f\"Precision: {cnn_model_report['precision']:.3f}\")\n",
        "print(cnn_model_report['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qj5rL5BP7lu8",
      "metadata": {
        "id": "qj5rL5BP7lu8"
      },
      "source": [
        "#### 2:\n",
        "LEARNING_RATE_2 = 0.0004  \n",
        "BATCH_SIZE_2 = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jae2c58JgeRo",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jae2c58JgeRo"
      },
      "outputs": [],
      "source": [
        "train_dataset = Twitter(train_df, w2v_model, sequence_len_1)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3mccYAV77lu8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3mccYAV77lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN(input_size,sequence_len_1, sequence_len_2, output_size, int(3 * sequence_len_1 * sequence_len_2 * 0.5))\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kvoFwCJ27lu8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kvoFwCJ27lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model_train_history = train_model(\n",
        "    model=cnn_model,\n",
        "    batch_size=BATCH_SIZE_2,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    epochs=EPOCHS,\n",
        "    train_set=train_dataset,\n",
        "    valid_set=valid_dataset,\n",
        "    device='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d-P3WGUt7lu8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d-P3WGUt7lu8"
      },
      "outputs": [],
      "source": [
        "trend_plot_helper(\n",
        "    {\n",
        "        \"Accuracy - Epoch\": [\n",
        "            (\"Train Acc\", cnn_model_train_history[\"train_acc\"]),\n",
        "            (\"Validation Acc\", cnn_model_train_history[\"valid_acc\"]),\n",
        "        ],\n",
        "        \"Loss - Epoch\": [\n",
        "            (\"Train Loss\", cnn_model_train_history[\"train_loss\"]),\n",
        "            (\"Validation Loss\", cnn_model_train_history[\"valid_loss\"])\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nHq8Jyvh7lu8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHq8Jyvh7lu8"
      },
      "outputs": [],
      "source": [
        "cnn_model_report = generate_confusion_matrix(\n",
        "    model=cnn_model,\n",
        "    dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eq0ZxyYo7lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eq0ZxyYo7lu9"
      },
      "outputs": [],
      "source": [
        "print(f\"Recall:    {cnn_model_report['recall']:.3f}\")\n",
        "print(f\"F1:        {cnn_model_report['f1']:.3f}\")\n",
        "print(f\"Precision: {cnn_model_report['precision']:.3f}\")\n",
        "print(cnn_model_report['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bVAjt617lu9",
      "metadata": {
        "id": "6bVAjt617lu9"
      },
      "source": [
        "#### 3:\n",
        "LEARNING_RATE_3 = 0.001   \n",
        "BATCH_SIZE_3 = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JBsZ0d0jgf4p",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JBsZ0d0jgf4p"
      },
      "outputs": [],
      "source": [
        "train_dataset = Twitter(train_df, w2v_model, sequence_len_1)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XEkv9U3B7lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XEkv9U3B7lu9"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN(input_size,sequence_len_1, sequence_len_2, output_size, int(3 * sequence_len_1 * sequence_len_2 * 0.5))\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OpJITOB97lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OpJITOB97lu9"
      },
      "outputs": [],
      "source": [
        "cnn_model_train_history = train_model(\n",
        "    model=cnn_model,\n",
        "    batch_size=BATCH_SIZE_3,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    epochs=EPOCHS,\n",
        "    train_set=train_dataset,\n",
        "    valid_set=valid_dataset,\n",
        "    device='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uU9gVjPO7lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uU9gVjPO7lu9"
      },
      "outputs": [],
      "source": [
        "trend_plot_helper(\n",
        "    {\n",
        "        \"Accuracy - Epoch\": [\n",
        "            (\"Train Acc\", cnn_model_train_history[\"train_acc\"]),\n",
        "            (\"Validation Acc\", cnn_model_train_history[\"valid_acc\"]),\n",
        "        ],\n",
        "        \"Loss - Epoch\": [\n",
        "            (\"Train Loss\", cnn_model_train_history[\"train_loss\"]),\n",
        "            (\"Validation Loss\", cnn_model_train_history[\"valid_loss\"])\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gmKtsJst7lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gmKtsJst7lu9"
      },
      "outputs": [],
      "source": [
        "cnn_model_report = generate_confusion_matrix(\n",
        "    model=cnn_model,\n",
        "    dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mubwTkGs7lu9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mubwTkGs7lu9"
      },
      "outputs": [],
      "source": [
        "print(f\"Recall:    {cnn_model_report['recall']:.3f}\")\n",
        "print(f\"F1:        {cnn_model_report['f1']:.3f}\")\n",
        "print(f\"Precision: {cnn_model_report['precision']:.3f}\")\n",
        "print(cnn_model_report['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsylvEQ48P-u",
      "metadata": {
        "id": "TsylvEQ48P-u"
      },
      "source": [
        "### Analyzing Our Outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i5qnhqldD2Bk",
      "metadata": {
        "id": "i5qnhqldD2Bk"
      },
      "source": [
        "\n",
        "\n",
        "##### Configuration 1\n",
        "- **Learning Rate**: 0.0004\n",
        "- **Batch Size**: 64\n",
        "\n",
        "##### Configuration 2\n",
        "- **Learning Rate**: 0.0004\n",
        "- **Batch Size**: 128\n",
        "\n",
        "##### Configuration 3\n",
        "- **Learning Rate**: 0.001\n",
        "- **Batch Size**: 64\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Optimal Configuration\n",
        "\n",
        "Based on the observed training and validation metrics, here are some insights:\n",
        "\n",
        "- **Batch Size**: Both batch sizes (64 and 128) provide similar overall accuracy, but batch size 64 offers slightly better recall and F1-score in combination with the higher learning rate (0.001).\n",
        "- **Learning Rate**: A higher learning rate (0.001) with batch size 64 seems to provide a slight improvement in performance, suggesting that this combination allows the model to learn more effectively.\n",
        "\n",
        "\n",
        "\n",
        "- The optimal configuration based on our results appears to be **Learning Rate: 0.001 and Batch Size: 64**. This setup provides the best balance of precision, recall, and F1-score, indicating effective learning and good generalization.\n",
        "- Different configurations of batch sizes and learning rates can significantly impact the training dynamics and final performance of our CNN model. It’s crucial to experiment with these hyperparameters to find the optimal setup for our specific dataset and task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sEyvBLoH-aFS",
      "metadata": {
        "id": "sEyvBLoH-aFS"
      },
      "source": [
        "### 2: Context Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FUYKbI6wjNXr",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FUYKbI6wjNXr"
      },
      "outputs": [],
      "source": [
        "input_size = 300\n",
        "output_size = 2\n",
        "sequence_len_1 = 196\n",
        "sequence_len_2 = 196 * 2\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE_3 = 0.001\n",
        "BATCH_SIZE_3 = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPBoVyEMjNXr",
      "metadata": {
        "id": "TPBoVyEMjNXr"
      },
      "outputs": [],
      "source": [
        "train_dataset = Twitter(train_df, w2v_model, sequence_len_1)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q57jlMI6jNXr",
      "metadata": {
        "id": "q57jlMI6jNXr"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN(input_size,sequence_len_1, sequence_len_2, output_size, int(3 * sequence_len_1 * sequence_len_2 * 0.5))\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5VlkuyLjNXr",
      "metadata": {
        "id": "r5VlkuyLjNXr"
      },
      "outputs": [],
      "source": [
        "cnn_model_train_history = train_model(\n",
        "    model=cnn_model,\n",
        "    batch_size=BATCH_SIZE_3,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    epochs=EPOCHS,\n",
        "    train_set=train_dataset,\n",
        "    valid_set=valid_dataset,\n",
        "    device='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W-pD8YUQjNXr",
      "metadata": {
        "id": "W-pD8YUQjNXr"
      },
      "outputs": [],
      "source": [
        "trend_plot_helper(\n",
        "    {\n",
        "        \"Accuracy - Epoch\": [\n",
        "            (\"Train Acc\", cnn_model_train_history[\"train_acc\"]),\n",
        "            (\"Validation Acc\", cnn_model_train_history[\"valid_acc\"]),\n",
        "        ],\n",
        "        \"Loss - Epoch\": [\n",
        "            (\"Train Loss\", cnn_model_train_history[\"train_loss\"]),\n",
        "            (\"Validation Loss\", cnn_model_train_history[\"valid_loss\"])\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mtXev9ddjNXs",
      "metadata": {
        "id": "mtXev9ddjNXs"
      },
      "outputs": [],
      "source": [
        "cnn_model_report = generate_confusion_matrix(\n",
        "    model=cnn_model,\n",
        "    dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dOIstA-VjNXs",
      "metadata": {
        "id": "dOIstA-VjNXs"
      },
      "outputs": [],
      "source": [
        "print(f\"Recall:    {cnn_model_report['recall']:.3f}\")\n",
        "print(f\"F1:        {cnn_model_report['f1']:.3f}\")\n",
        "print(f\"Precision: {cnn_model_report['precision']:.3f}\")\n",
        "print(cnn_model_report['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ZRGwfPMEnH-",
      "metadata": {
        "id": "-ZRGwfPMEnH-"
      },
      "source": [
        "### 13.     In your opinion, what are the advantages and disadvantages of increasing the size of the context window, so that it is larger than the entire dataset, in a convolutional neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V5AkTS4zEqyN",
      "metadata": {
        "id": "V5AkTS4zEqyN"
      },
      "source": [
        "In a convolutional neural network (CNN), the context window refers to the receptive field or the region of the input data that the network considers at a time. Increasing the size of the context window, especially to a size larger than the entire dataset, can have significant implications. Here are the advantages and disadvantages of such an approach:\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "1. **Global Context Awareness**:\n",
        "   - **Enhanced Feature Extraction**: A larger context window allows the network to consider the entire input at once, capturing global features and relationships that span the entire dataset. This can be beneficial for tasks where understanding the overall structure or context is crucial (e.g., scene recognition, sentiment analysis in long text sequences).\n",
        "   - **Better Generalization**: By considering the entire dataset, the network might learn more generalized features that are not limited to local patterns. This could potentially improve the model's ability to generalize to new, unseen data.\n",
        "\n",
        "2. **Reduced Information Loss**:\n",
        "   - **Comprehensive View**: Larger context windows ensure that no part of the input is left out during the feature extraction process. This comprehensive view can be particularly useful in scenarios where every detail matters (e.g., medical imaging).\n",
        "\n",
        "3. **Potential for Improved Performance**:\n",
        "   - **Context-Rich Decisions**: With access to a larger context, the network can make more informed decisions. This can lead to improved accuracy and performance on tasks that benefit from understanding long-range dependencies.\n",
        "\n",
        "#### Disadvantages\n",
        "\n",
        "1. **Increased Computational Complexity**:\n",
        "   - **Higher Resource Requirements**: A larger context window significantly increases the computational load. This includes more memory usage and longer training times, as the network needs to process larger amounts of data simultaneously.\n",
        "   - **Inefficiency**: Processing the entire dataset at once can be inefficient, especially if the dataset is very large. It can lead to slow training and inference times, making the approach impractical for real-time applications.\n",
        "\n",
        "2. **Risk of Overfitting**:\n",
        "   - **Overfitting to Global Features**: By focusing on global features, the network might overfit to the specific patterns present in the training dataset. This could result in poor performance on new data that has different global patterns.\n",
        "   - **Reduced Robustness**: The model may become less robust to variations in local patterns, as it might rely too heavily on global context.\n",
        "\n",
        "3. **Complexity in Implementation**:\n",
        "   - **Architecture Adjustments**: Increasing the context window size might require significant changes to the network architecture, including deeper or wider layers to handle the larger input effectively.\n",
        "   - **Hyperparameter Tuning**: More extensive hyperparameter tuning might be necessary to balance the increased complexity and ensure effective learning.\n",
        "\n",
        "4. **Diminishing Returns**:\n",
        "   - **Marginal Gains**: After a certain point, increasing the context window size may result in diminishing returns. The additional context might not provide substantial new information beyond what is already captured, leading to unnecessary computational overhead without corresponding improvements in performance.\n",
        "\n",
        "\n",
        "Increasing the size of the context window in a CNN can provide advantages such as enhanced feature extraction, better generalization, and reduced information loss. However, it also comes with significant drawbacks, including increased computational complexity, risk of overfitting, implementation challenges, and potential diminishing returns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tthwbahpGSjg",
      "metadata": {
        "id": "tthwbahpGSjg"
      },
      "source": [
        "#### Analysis\n",
        "We can see that increasing the context window size to 196 significantly increases the computational complexity of our model. However, the performance metrics show no significant improvement over the smaller context window size of 64. The results are quite similar, indicating that the larger context window does not provide additional benefits in terms of model accuracy, recall, precision, and F1-score.\n",
        "\n",
        "Given this observation, the increased computational cost associated with the larger context window size does not justify its use. Instead, similar improvements in model performance could potentially be achieved through fine-tuning our model with the smaller context window size of 64. This approach would be more efficient and practical, allowing us to achieve high performance without the added computational burden.\n",
        "\n",
        "By focusing on fine-tuning the model, we can optimize hyperparameters, adjust learning rates, and implement regularization techniques to enhance the model's effectiveness while maintaining computational efficiency. This strategy ensures that we leverage the strengths of the smaller context window size while avoiding unnecessary increases in computation time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8UHbOPX6-bmr",
      "metadata": {
        "id": "8UHbOPX6-bmr"
      },
      "source": [
        "### 3: Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jTBQHW0IYEZE",
      "metadata": {
        "id": "jTBQHW0IYEZE"
      },
      "outputs": [],
      "source": [
        "class CNN_2(nn.Module):\n",
        "    def __init__(self, input_size = 300, sequence_len_1_ = 64, sequence_len_2_ = 128, output_size = 2, flatt_size = 12288,  dropout_prob=0.5):\n",
        "        super(CNN_2, self).__init__()\n",
        "        self.conv1_1 = nn.Conv1d(input_size, sequence_len_1, kernel_size=3, padding=1)\n",
        "        self.conv1_2 = nn.Conv1d(input_size, sequence_len_1, kernel_size=5, padding=2)\n",
        "        self.conv1_3 = nn.Conv1d(input_size, sequence_len_1, kernel_size=7, padding=3)\n",
        "\n",
        "        self.conv2_1 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=5, padding=2)\n",
        "        self.conv2_3 = nn.Conv1d(sequence_len_1, sequence_len_2, kernel_size=7, padding=3)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(flatt_size, sequence_len_2)\n",
        "        self.fc2 = nn.Linear(sequence_len_2, output_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(sequence_len_2)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(output_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.relu(self.conv1_1(x))\n",
        "        x2 = self.relu(self.conv1_2(x))\n",
        "        x3 = self.relu(self.conv1_3(x))\n",
        "\n",
        "        x1 = self.pool(self.relu(self.conv2_1(x1)))\n",
        "        x2 = self.pool(self.relu(self.conv2_2(x2)))\n",
        "        x3 = self.pool(self.relu(self.conv2_3(x3)))\n",
        "\n",
        "        x_concatenated = torch.cat((x1, x2, x3), dim=1)\n",
        "\n",
        "        x = x_concatenated.view(x_concatenated.size(0), -1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        x = self.batchnorm1(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        x = self.batchnorm2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SrtTWnlajcnh",
      "metadata": {
        "id": "SrtTWnlajcnh"
      },
      "outputs": [],
      "source": [
        "input_size = 300\n",
        "output_size = 2\n",
        "sequence_len_1 = 64\n",
        "sequence_len_2 = 128\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE_3 = 0.001\n",
        "BATCH_SIZE_3 = 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xXanZQCJjcnr",
      "metadata": {
        "id": "xXanZQCJjcnr"
      },
      "outputs": [],
      "source": [
        "train_dataset = Twitter(train_df, w2v_model, sequence_len_1)\n",
        "valid_dataset = Twitter(valid_df, w2v_model, sequence_len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nbYGGzYulUkg",
      "metadata": {
        "id": "nbYGGzYulUkg"
      },
      "outputs": [],
      "source": [
        "cnn_model = CNN_2(input_size,sequence_len_1, sequence_len_2, output_size, int(3 * sequence_len_1 * sequence_len_2 * 0.5) ,0.5)\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GjJTIvJZlX0a",
      "metadata": {
        "id": "GjJTIvJZlX0a"
      },
      "outputs": [],
      "source": [
        "cnn_model_train_history = train_model(\n",
        "    model=cnn_model,\n",
        "    batch_size=BATCH_SIZE_3,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    epochs=EPOCHS,\n",
        "    train_set=train_dataset,\n",
        "    valid_set=valid_dataset,\n",
        "    device='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ubGx3DPllZ3R",
      "metadata": {
        "id": "ubGx3DPllZ3R"
      },
      "outputs": [],
      "source": [
        "trend_plot_helper(\n",
        "    {\n",
        "        \"Accuracy - Epoch\": [\n",
        "            (\"Train Acc\", cnn_model_train_history[\"train_acc\"]),\n",
        "            (\"Validation Acc\", cnn_model_train_history[\"valid_acc\"]),\n",
        "        ],\n",
        "        \"Loss - Epoch\": [\n",
        "            (\"Train Loss\", cnn_model_train_history[\"train_loss\"]),\n",
        "            (\"Validation Loss\", cnn_model_train_history[\"valid_loss\"])\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YaIyy7O3lcI7",
      "metadata": {
        "id": "YaIyy7O3lcI7"
      },
      "outputs": [],
      "source": [
        "cnn_model_report = generate_confusion_matrix(\n",
        "    model=cnn_model,\n",
        "    dataset=valid_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qEu9MAGdjcns",
      "metadata": {
        "id": "qEu9MAGdjcns"
      },
      "outputs": [],
      "source": [
        "print(f\"Recall:    {cnn_model_report['recall']:.3f}\")\n",
        "print(f\"F1:        {cnn_model_report['f1']:.3f}\")\n",
        "print(f\"Precision: {cnn_model_report['precision']:.3f}\")\n",
        "print(cnn_model_report['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJMZERgTHKN5",
      "metadata": {
        "id": "qJMZERgTHKN5"
      },
      "source": [
        "By adding Dropout and Batch Normalization layers to the neural network, we observed improvements in the model's performance metrics, including recall, F1-score, precision, and accuracy. These regularization methods helped reduce overfitting, leading to better generalization and robustness to variations in input data.\n",
        "\n",
        "To further optimize the architecture, additional experimentation with different dropout rates and batch normalization configurations can be conducted. Comparing these results with previous ones clearly demonstrates the benefits of incorporating regularization techniques in neural network training."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I2WGNNJxMouq",
        "gWLJAqMiNCy4",
        "gxWrUvOdNEdG",
        "eY99YE-VG1C9",
        "3e83a8b72b644c2",
        "32b050729991fec9",
        "57f7552c58ea498e",
        "MrLkfaUYp0Gu",
        "JQhAQ05Cp8s-",
        "8uLFStMGqF-G",
        "24k5lCShsI17",
        "eH7D5kpSshyi",
        "OxZpPQCestfy",
        "EJdB_24P-YfZ",
        "2e129c7a31663741",
        "21730f8dd01e5b4e",
        "PB6Pbrn5xfXB",
        "13h5Re2ATU78",
        "QRKFqGLSTbx-",
        "bvNy5ErLlnrq",
        "iPxXZPGxvbF5",
        "670fcd8db0e5651c",
        "aa29012e257e3d9",
        "1566b2a8428318c8",
        "Yt9SMAxCv4ak",
        "d8cONKXFzdJ4",
        "RAlDfhIh78Z-",
        "TsylvEQ48P-u",
        "i5qnhqldD2Bk",
        "-ZRGwfPMEnH-",
        "V5AkTS4zEqyN",
        "tthwbahpGSjg"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}